# name: source端连接器名称，debezium的原生参数，无默认值，可自定义，不同连接器需保证名称的唯一性
name=mysql-sink
# connector.class: 连接器的启动类，debezium的原生参数，无默认值，示例中的值为mysql sink connector对应的类名，不可修改
connector.class=io.debezium.connector.mysql.sink.MysqlSinkConnector
# tasks.max: 连接器创建的最大任务数，debezium的原生参数，默认值为1，MySQL connector通常为单任务，不建议修改
tasks.max=1
# topics: sink端从kafka抽取数据的topic，新增参数，String类型，无默认值
# 与mysql-source.properties的配置项transforms.route.replacement相对应
topics=mysql_server_topic
# opengauss.driver: openGauss驱动名，新增参数，String类型，无默认值，与JDBC驱动相对应，不可修改
opengauss.driver=org.opengauss.Driver
# opengauss.username: openGauss用户名，新增参数，String类型，无默认值，根据实际自定义修改
opengauss.username=test
# opengauss.password: openGauss用户密码，新增参数，String类型，无默认值，根据实际自定义修改
opengauss.password=**********
# opengauss.url: openGauss连接url，新增参数，String类型，无默认值，根据实际自定义修改，包括数据库ip，端口号，连接的数据库
opengauss.url=jdbc:opengauss://127.0.0.1:5432/postgres?loggerLevel=OFF
# parallel.replay.thread.num: 并行回放默认线程数量，新增参数，int类型，默认为30，可自定义修改，取值需大于0
parallel.replay.thread.num=30
# xlog.location: 增量迁移停止时openGauss端lsn的存储文件路径，新增参数，String类型，无默认值，根据实际自定义修改，需保证文件有读写权限
xlog.location=/tmp/xlog.txt
# schema.mappings: mysql和openGauss的schema映射关系，与全量迁移chameleon配置相对应，新增参数，String类型，无默认值
# 用；区分不同的映射关系，用：区分mysql的database和openGauss的schema
# 例如chameleon的配置
# schema_mappings:
#   mysql_database1: opengauss_schema1
#   mysql_database2: opengauss_schema2
# 则sink端的schema.mappings参数需配置为
# schema.mappings=mysql_database1:opengauss_schema1;mysql_database2:opengauss_schema2
schema.mappings=mysql_database1:opengauss_schema1;mysql_database2:opengauss_schema2
# commit.process.while.running：布尔值，默认为false，通过该配置项选择是否上报迁移进度
commit.process.while.running=true
# sink.process.file.path：迁移进度文件的输出路径，只有commit.process.while.running=true时才起作用，默认在迁移插件同一目录下
sink.process.file.path=/***/***/***/
# commit.time.interval：迁移进度上报的时间间隔，取int型整数，默认为1，单位：s
commit.time.interval=1
# fail.sql.path：回放失败的sql语句输出路径，默认在迁移插件同一目录下
fail.sql.path=/***/***/***/
# create.count.info.path：记录源端日志生产起始点的文件读取路径，需与source端的此配置项相同，默认与迁移插件在同一目录下
create.count.info.path=/***/***/***/
# process.file.count.limit：进度目录下文件数目限制值，如果进度目录下的文件数目超过该值，工具启动时会按时间从早到晚删除多余的文件，默认值为10
process.file.count.limit=10
# process.file.time.limit：进度文件保存时长，超过该时长的文件会在工具下次启动时删除，默认值为168，单位：小时
process.file.time.limit=168
# append.write：进度文件写入方式，true表示追加写入，false表示覆盖写入，默认值为false
append.write=false
# file.size.limit：文件大小限制，超过该限制值工具会另启新文件写入，默认为10，单位：兆
file.size.limit=10
# queue.size.limit: 存储kafka记录的队列的最大长度，新增参数，int类型，默认值为1000000，可自定义
queue.size.limit = 1000000
# open.flow.control.threshold: 用于流量控制，新增参数，double类型，默认值为0.8，可自定义
# 当存储kafka记录的队列长度>最大长度queue.size.limit*该门限值时，将启用流量控制，暂停从kafka抽取数据
open.flow.control.threshold = 0.8
# close.flow.control.threshold: 用于流量控制，新增参数，double类型，默认值为0.7，可自定义
# 当存储kafka记录的队列长度<最大长度queue.size.limit*该门限值时，将关闭流量控制，继续从kafka抽取数据
close.flow.control.threshold = 0.7
#breakpoint.kafka.topic:断点记录kafka消息topic,可根据实际情况修改，默认值为bp_topic
record.breakpoint.kafka.topic=bp_topic
#breakpoint.kafka..recovery.attempts:从kafka读取断点topic的最大重试次数
record.breakpoint.kafka.attempts=3
#breakpoint.kafka..recovery.bootstrap.servers:kafka服务器地址，可根据实际情况修改，默认值为localhost:9092
record.breakpoint.kafka.bootstrap.servers=localhost:9092
#record.breakpoint.kafka.size.limit：断点记录Kafka的条数限制，超过该限制会触发删除Kafka的断点清除策略，删除无用的断点记录数据，单位：事务万条数
record.breakpoint.kafka.size.limit=3000
#record.breakpoint.kafka.clear.interval：断点记录Kafka的时间限制，超过该限制会触发删除Kafka的断点清除策略，删除无用的断点记录数据，单位：小时
record.breakpoint.kafka.clear.interval=1