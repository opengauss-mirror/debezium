# name: sink端连接器名称
name=connect-postgres-sink
# 连接器启动类
connector.class=io.debezium.connector.postgresql.sink.PostgresSinkConnector
# topics: sink端从kafka抽取数据的topic，与postgres-source.properties配置项transforms.route.replacement对应
topics=postgres_server_topic
# max.thread.count: 最大并发线程数
max.thread.count=50
# database.username: 数据库用户名
database.username=username
# database.password: 数据库用户密码
database.password=*******
# database.ip: 数据库ip
database.ip=127.0.0.1
# database.port: 数据库端口
database.port=15000
# database.name: 数据库名称
database.name=testdb
# schema.mappings=postgres_schema1:opengauss_schema1;postgres_schema2:opengauss_schema2
schema.mappings=public:public
# xlog.location 增量迁移停止时openGauss端lsn的存储文件路径，新增参数，String类型，无默认值，根据实际自定义修改，需保证文件有读写权限
xlog.location=/***/***/***/
# commit.process.while.running：布尔值，默认为false，通过该配置项选择是否上报迁移进度
commit.process.while.running=false
# sink.process.file.path：迁移进度文件的输出路径，只有commit.process.while.running=true时才起作用，默认在迁移插件同一目录下
sink.process.file.path=/***/***/***/
# commit.time.interval：迁移进度上报的时间间隔，取int型整数，默认为1，单位：s
commit.time.interval=1
# fail.sql.path：回放失败的sql语句输出路径，默认在迁移插件同一目录下
fail.sql.path=/***/***/***/
# create.count.info.path：记录源端有效日志生产总数的文件读取路径，需与source端的此配置项相同，默认与迁移插件在同一目录下
create.count.info.path=/***/***/***/
# process.file.count.limit：进度目录下文件数目限制值，如果进度目录下的文件数目超过该值，工具启动时会按时间从早到晚删除多余的文件，默认值为10
process.file.count.limit=10
# process.file.time.limit：进度文件保存时长，超过该时长的文件会在工具下次启动时删除，默认值为168，单位：小时
process.file.time.limit=168
# append.write：进度文件写入方式，true表示追加写入，false表示覆盖写入，默认值为false
append.write=false
# file.size.limit：文件大小限制，超过该限制值工具会另启新文件写入，默认为10，单位：兆
file.size.limit=10
# queue.size.limit: 存储kafka记录的队列的最大长度，新增参数，int类型，默认值为1000000，可自定义
queue.size.limit=1000000
# open.flow.control.threshold: 用于流量控制，新增参数，double类型，默认值为0.8，可自定义
# 当存储kafka记录的队列长度或某一个按表并发线程中预处理数据的队列长度>最大长度queue.size.limit*该门限值时，将启用流量控制，暂停从kafka抽取数据
open.flow.control.threshold=0.8
# close.flow.control.threshold: 用于流量控制，新增参数，double类型，默认值为0.7，可自定义
# 当存储kafka记录的队列长度和所有按表并发线程中预处理数据的队列长度<最大长度queue.size.limit*该门限值时，将关闭流量控制，继续从kafka抽取数据
close.flow.control.threshold=0.7
# 控制在全量数据迁移处理完csv文件后是否删除文件，默认值false.
delete.full.csv.file=false
# wait.timeout.second：sink端数据库停止服务后迁移工具等待数据库恢复服务的最大时长，默认值：28800，单位：秒
wait.timeout.second=28800
# 建表添加with选项，JsonArray形式，table为表名，with选项值之间用","隔开，默认值：[]
create.table.with.options=[{"table":"t1", "with":"fillfactor=70,compression='no'"}, {"table":"t2", "with":"fillfactor=80,compression='no'"}]