# name: sink端连接器名称
name=connect-opengauss-sink
# 连接器启动类
connector.class=io.debezium.connector.opengauss.sink.OpengaussSinkConnector
# topics: sink端从kafka抽取数据的topic，与opengauss-source.properties配置项transforms.route.replacement对应
topics=dml_topic
# max.thread.count: 最大并发线程数
max.thread.count=50
# database.type: 数据库类型，当前支持mysql，opengauss和oracle，默认值为mysql
database.type=mysql
# database.username: 数据库用户名
database.username=mysql_user
# database.password: 数据库用户密码
database.password=*****
# database.ip: 数据库ip
database.ip=127.0.0.1
# database.port: 数据库端口
database.port=3306
# database.name: 数据库名称
database.name=mysql
# schema.mappings=opengauss_schema1:mysql_database1;opengauss_schema2:mysql_database2
schema.mappings=opengauss_schema1:mysql_database1;opengauss_schema2:mysql_database2
# commit.process.while.running：布尔值，默认为false，通过该配置项选择是否上报迁移进度
commit.process.while.running=true
# sink.process.file.path：迁移进度文件的输出路径，只有commit.process.while.running=true时才起作用，默认在迁移插件同一目录下
sink.process.file.path=/***/***/***/
# commit.time.interval：迁移进度上报的时间间隔，取int型整数，默认为1，单位：s
commit.time.interval=1
# fail.sql.path：回放失败的sql语句输出路径，默认在迁移插件同一目录下
fail.sql.path=/***/***/***/
# create.count.info.path：记录源端有效日志生产总数的文件读取路径，需与source端的此配置项相同，默认与迁移插件在同一目录下
create.count.info.path=/***/***/***/
# process.file.count.limit：进度目录下文件数目限制值，如果进度目录下的文件数目超过该值，工具启动时会按时间从早到晚删除多余的文件，默认值为10
process.file.count.limit=10
# process.file.time.limit：进度文件保存时长，超过该时长的文件会在工具下次启动时删除，默认值为168，单位：小时
process.file.time.limit=168
# append.write：进度文件写入方式，true表示追加写入，false表示覆盖写入，默认值为false
append.write=false
# file.size.limit：文件大小限制，超过该限制值工具会另启新文件写入，默认为10，单位：兆
file.size.limit=10
# queue.size.limit: 存储kafka记录的队列的最大长度，新增参数，int类型，默认值为1000000，可自定义
queue.size.limit=1000000
# open.flow.control.threshold: 用于流量控制，新增参数，double类型，默认值为0.8，可自定义
# 当存储kafka记录的队列长度或某一个按表并发线程中预处理数据的队列长度>最大长度queue.size.limit*该门限值时，将启用流量控制，暂停从kafka抽取数据
open.flow.control.threshold=0.8
# close.flow.control.threshold: 用于流量控制，新增参数，double类型，默认值为0.7，可自定义
# 当存储kafka记录的队列长度和所有按表并发线程中预处理数据的队列长度<最大长度queue.size.limit*该门限值时，将关闭流量控制，继续从kafka抽取数据
close.flow.control.threshold=0.7
# 控制在全量数据迁移处理完csv文件后是否删除文件，默认值false.
delete.full.csv.file=false
#record.breakpoint.kafka.topic:断点记录kafka消息topic,可根据实际情况修改，默认值为bp_topic
record.breakpoint.kafka.topic=bp_topic
#record.breakpoint.kafka.attempts:从kafka读取断点topic的最大重试次数
record.breakpoint.kafka.attempts=3
#record.breakpoint.kafka.bootstrap.servers:kafka服务器地址，可根据实际情况修改，默认值为localhost:9092
record.breakpoint.kafka.bootstrap.servers=localhost:9092
#record.breakpoint.kafka.size.limit：断点记录Kafka的条数限制，超过该限制会触发删除Kafka的断点清除策略，删除无用的断点记录数据，单位：事务万条数
record.breakpoint.kafka.size.limit=3000
#record.breakpoint.kafka.clear.interval：断点记录Kafka的时间限制，超过该限制会触发删除Kafka的断点清除策略，删除无用的断点记录数据，单位：小时
record.breakpoint.kafka.clear.interval=1
# record.breakpoint.repeat.count.limit：断点续传时，查询待回放数据是否已在断点之前备回放的数据条数，默认值：50000
record.breakpoint.repeat.count.limit=50000
# wait.timeout.second：sink端数据库停止服务后迁移工具等待数据库恢复服务的最大时长，默认值：28800，单位：秒
wait.timeout.second=28800